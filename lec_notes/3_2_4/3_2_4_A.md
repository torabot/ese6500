Sure — “Way A” is the **batch (all-at-once)** derivation. I’ll do it from the ground up, with the exact objects the section uses: stacking the (k) measurements into a vector and solving a least-squares / Gaussian problem. 

---

## 0) The model (what we’re assuming)

You measure the same unknown scalar (x) repeatedly:

[
y_i = x + \nu_i,\quad \nu_i \sim \mathcal N(0,1),\quad i=1,\dots,k,
]

and the noises are independent. 

---

## 1) Stack it into a single linear system

Define

[
y \triangleq
\begin{bmatrix}
y_1\ \vdots\ y_k
\end{bmatrix}
\in\mathbb R^{k\times 1},\qquad
\nu \triangleq
\begin{bmatrix}
\nu_1\ \vdots\ \nu_k
\end{bmatrix},\qquad
C \triangleq
\begin{bmatrix}
1\ \vdots\ 1
\end{bmatrix}
\in\mathbb R^{k\times 1}.
]

Then all (k) equations become one vector equation:

[
y = Cx + \nu.
]

Because each (\nu_i\sim\mathcal N(0,1)) and independent, the stacked noise is

[
\nu \sim \mathcal N(0, I_k).
]

That’s the batch measurement model. 

---

## 2) Write the likelihood (p(y\mid x))

Given (x), (y) is Gaussian:

[
p(y\mid x) = \mathcal N(Cx,, I_k).
]

So the (negative) log-likelihood is, up to constants:

[
-\log p(y\mid x) ;=; \tfrac12 (y-Cx)^\top I_k^{-1}(y-Cx)
= \tfrac12 |y-Cx|^2.
]

So maximizing likelihood is the same as minimizing the least-squares cost

[
J(x) = |y-Cx|^2.
]

---

## 3) Solve the least-squares problem (normal equations)

Expand (J(x)):

[
J(x)=(y-Cx)^\top(y-Cx) = y^\top y - 2x, C^\top y + x^2, C^\top C.
]

Differentiate w.r.t. scalar (x) and set to zero:

[
\frac{dJ}{dx} = -2 C^\top y + 2x, C^\top C = 0
\quad\Rightarrow\quad
x = (C^\top C)^{-1} C^\top y.
]

So the batch estimate is

[
\boxed{\hat x_k = (C^\top C)^{-1}C^\top y.}
]

Now plug in the special (C=\mathbf 1_k):

* (C^\top C = \mathbf 1_k^\top \mathbf 1_k = k)
* (C^\top y = \sum_{i=1}^k y_i)

Therefore

[
\boxed{\hat x_k = \frac{1}{k}\sum_{i=1}^k y_i,}
]

which is the sample mean result shown in the section. 

---

## 4) Derive the covariance of (\hat x_k)

From the formula we just got,

[
\hat x_k = (C^\top C)^{-1}C^\top y
= (C^\top C)^{-1}C^\top (Cx+\nu)
= x + (C^\top C)^{-1}C^\top \nu.
]

So the estimation error is

[
\tilde x_k \triangleq \hat x_k - x = (C^\top C)^{-1}C^\top \nu.
]

Take variance (covariance, but it’s scalar):

[
\mathrm{Var}(\hat x_k)
= \mathrm{Var}(\tilde x_k)
= (C^\top C)^{-1} C^\top ,\mathrm{Cov}(\nu), C, (C^\top C)^{-1}.
]

But (\mathrm{Cov}(\nu)=I_k), so

[
\mathrm{Var}(\hat x_k)
= (C^\top C)^{-1} C^\top I_k C (C^\top C)^{-1}
= (C^\top C)^{-1}.
]

And since (C^\top C = k),

[
\boxed{\mathrm{Var}(\hat x_k)=\frac{1}{k}.}
]

That’s exactly the “covariance shrinks like (1/k)” punchline. 

---

### Tiny intuition (to make it stick)

Averaging (k) independent noises divides the variance by (k). This batch derivation is just that fact wearing a linear-algebra costume.

If you want, I can also do the same derivation when each measurement has variance (\sigma^2) (then (I_k) becomes (\sigma^2 I_k) and you’ll see (\mathrm{Var}(\hat x_k)=\sigma^2/k)).

